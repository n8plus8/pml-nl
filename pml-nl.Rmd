---
title: "(HAR) Weight Lifting Exercises Dataset"
author: "N. Leung"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, message = FALSE, comment = NA)
```

```{r}
load(file = "pml-nl.RData"); library(caret)
```

## **Introduction**
Devices such as Jawbone Up, Nike FuelBand and Fitbit now make it possible to collect a large amount of data about personal activity relatively inexpensively. These type of devices are part of the quantified self movement - a group of enthusiasts who take measurements about themselves regularly to improve their health, to find patterns in their behavior, or because they are tech geeks. One thing that people regularly do is quantify how much of a particular activity they do, but they rarely quantify *how well they do it*.


## **Summary**
6 participants were asked to perform barbell lifts correctly and incorrectly in 5 different ways. Using data from accelerometers on the belt, forearm, arm and barbell, we will predict the manner in which they did the exercise. This is the "classe" variable data set, ordinally ranked A to E.  

More information is available from the website here: <http://groupware.les.inf.puc-rio.br/har> (see the section on the Weight Lifting Exercise Dataset).


10 models were built using the **'caret'** package. 10-fold cross validation, repeated 3 times, was performed for all models. In addition, a hyperparameter grid search was used to tune xgbTree. The **doSNOW** package was used for processing in parallel (optional).


1. Classification and Regression Tree (CART)
2. Flexible Discriminant Analysis (FDA)
3. Learning Vector Quantization (LVQ)
4. Linear Discriminant Analysis (LDA)
5. Quadratic Discriminant Analysis (QDA)
6. Random Forests (RF)
7. Regularized Discriminant Analysis (RDA)
8. eXtreme Gradient Boosting (XGB)	using xgbTree
9. Support Vector Machine (SVM) using Radial kernel
10. Stochastic Gradient Boosting (GBM)

The GBM model, with an accuracy of 0.9934, was chosen to predict from the test set. However, other models achieved similar accuracy and can be used for prediction.


#### *Note: In and Out of Sample Errors*  
The data set was not split into training and testing sets. In a real environment it would be prudent to do so. The motivation for this exercise was to experiment with the **caret** package, compare an exagerrated amount of models and use a majority vote if performance varied highly. All predictions passed the final quiz.  

Data splitting is performed using the **createDataPartition()** function in **caret**  
`set.seed(8)`   
`inTrain <- createDataPartition(trainData$classe, p = 3/4, list = FALSE)`  
`train <- trainData[inTrain, ]`  
`valid <- trainData[-inTrain, ]`  


## **Data Tidying**
The original training set contained 160 variables, many of which contained NAs and missing values. The final training set contains 54 variables. Variables removed:

* Indexes, names, and timestamps ("V1", * "user_name", "raw_timestamp_part_1", " raw_timestamp_part_2", "cvtd_timestamp")
* Majority (at least 95%) NAs and missing values
* Near zero variance variables


Code used for cleaning  
`View(training) # a lot of blanks and NAs`

`# "classe" is a factor`  
`training$classe <- as.factor(training$classe)`

`# remove the index, names, and timestamps`  
`f_train <- data.frame(training[ , -(1:5)]) # f, for filtered`

`# Dealing with NAs`  
`# Total NAs per column set to 95% threshold to keep columns where we may be able`  
`# to impute missing values later`  
`f_train <- f_train[ , colSums(!is.na(f_train)) > .95*nrow(f_train)]`  
`sum(is.na(f_train)) # looks like all columns had over 95% NAs`

`# Removing columns with more than 95% blanks`  
`empties <- apply(f_train, 2, function(x) sum(x == "")) > .95*nrow(f_train)`  
`f_train <- f_train[ , !empties]`  
`sum(f_train == "") # looks like all columns had over 95% blanks`  

`# We could have probably eliminated problematic variables with nearZeroVar()`  
`# Let's see what it tells us now`  
`nzv <- nearZeroVar(f_train[,-55])`  
`table(f_train[,1]) # drop it`  
`f_train <- f_train[ , -nzv]`



Other transformations we can perform:  
Remove correlated predictors, say if we wanted to build a partial least squares model.  
Normalize or log-transform the variables for PCA. We can also use *preProcess* in the train() function.  

Let's continue as is.

```{r, echo = TRUE}
# Class balances
table(training$classe)
```


## **Modeling**

**Setting seed, training options and hyperparameter grid for xgbTree**  
`set.seed(88888888)`  

`control <- trainControl(method = "repeatedcv", number = 10, repeats = 3)`  

`tune.grid <- expand.grid(eta = c(0.05, 0.075, 0.1),`  
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;`nrounds = c(50, 75, 100),`  
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;`max_depth = 6:8,`  
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;`min_child_weight = c(2.0, 2.25, 2.5),`  
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;`colsample_bytree = c(0.3, 0.4, 0.5),`  
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;`gamma = 0,`  
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;`subsample = 1)`  
                         
*Optional, **doSNOW** for parallel processing*  
`library(doSNOW); cl <- makeCluster(3, type = "SOCK")`&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Set # of clusters carefully!  
`registerDoSNOW(cl)`&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Register cluster  
`stopCluster(cl)`&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Remember to stop the cluster when the models are completed

**Building the models**  
Code for Stochastic Gradient Boosting (GBM) model. Other models use the same naming and coding convention.
`modelGBM <- train(classe ~ ., data = f_train, method = "gbm", trControl = control)`  


## **Collation And Visualization Of Results**
```{r, echo = TRUE}
results <- resamples(list(CART = modelCART,
                           FDA = modelFDA,
                           LVQ = modelLVQ,
                           LDA = modelLDA,
                           QDA = modelQDA,
                            RF = modelRF,
                           RDA = modelRDA,
                           XGB = modelXGB,
                           SVM = modelSVM,
                           GBM = modelGBM))
bwplot(results)
```

Boosting and random forests perform very well.  
You can also extract elements from a confusion matrix for the model of interest
```{r, echo = TRUE}
confusionMatrix(f_train$classe, predict(modelGBM))$overall[1]
```


## **Predictions**
```{r}
data.frame(XGB = predict(modelXGB, testing),
                          RF = predict(modelRF, testing),
                          GBM = predict(modelGBM, testing),
                          SVM = predict(modelSVM, testing),
                          FDA = predict(modelFDA, testing),
                          RDA = predict(modelRDA, testing),
                          QDA = predict(modelQDA, testing),
                          LDA = predict(modelLDA, testing),
                          LVQ = predict(modelLVQ, testing),
                          CART = predict(modelCART, testing))
```

Examine the full code at <https://github.com/n8plus8/pml-nl/blob/master/pml-nl.R>